{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU memory usage for deep learning image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的\n",
    "DLによる画像処理について、モデルの学習と実行に必要なGPUメモリ容量の理論値と実測値の差を検証する。\n",
    "\n",
    "### 方法\n",
    "以下の6つの項目について、メモリの使用量の理論値と実測値を比較する。\n",
    "   \n",
    "1. [Tensor variable](#tensor-variable)\n",
    "2. [Convolutional Neural Network](#convolutional-neural-network)\n",
    "3. [VGG16](#vgg16)\n",
    "4. [ResNet50](#resnet50)\n",
    "5. [Vision Transformer](#visiont-transformer)\n",
    "6. [UNETR](#unetr)\n",
    "\n",
    "メモリの使用量の実測値は、pytorchもしくはtensorflowの関数と、nvml（nvidia  management library）の関数で測定する。\n",
    "pytorchやtensorflowの関数ではプログラム中で確保された容量が測定される。一方、nvmlではプログラム実行に必要なpythonやcudaなどのオーバーヘッドを含んだ容量が測定される。\n",
    "変数については２つの実装方法（pytorch, tensorflow）で実測値を測定する。どちらの実装方法でも実測値に大きな差はないことが期待される。\n",
    "\n",
    "データセットには疑似乱数を用いる。データセットのサイズは、以下のようにする\n",
    "- For Tensor variable\n",
    "  - input = 0.25 GiB dimension (1GiB size with 4byte float32)\n",
    "- For CNN, VGG, ResNet, ViT\n",
    "  - input = [3,224,224]\n",
    "  - output = [10]\n",
    "  - datasize = 512\n",
    "  - batchsize = 128\n",
    "- For UNETR\n",
    "  - input = [1, 64,64,64]\n",
    "  - output = [2, 64, 64, 64]\n",
    "  - datasize = 32\n",
    "  - batchsize = 8\n",
    "\n",
    "### 結果\n",
    "|  | Estimated [MiB] | Real [MiB] | Error [%] |\n",
    "|---|---|---|---|\n",
    "| Variable | 1024 | 1024 | 0 |\n",
    "| CNN | 636 | 531 | 19.8 |\n",
    "| VGG16 | 15929 | 17232 | 7.6 |\n",
    "| ResNet50 | 11378 | 11738 | 3.1 |\n",
    "| ViT | 18678 | 17218 | 8.5 |\n",
    "| UNETR | 5577 | 5178 | 7.7 |\n",
    "\n",
    "#### 検証環境\n",
    "- python 3.10\n",
    "- tensorflow 2.13.0\n",
    "- torch 2.0.1\n",
    "- NVIDIA driver 530.30.02\n",
    "- CUDA toolkit 11.8\n",
    "- cuDNN 8.9\n",
    "\n",
    "#### 参考文献\n",
    "1. [Estimating GPU Memory Consumption of Deep Learning Models](https://2020.esec-fse.org/details/esecfse-2020-industry-papers/5/Estimating-GPU-Memory-Consumption-of-Deep-Learning-Models), [video](https://dl.acm.org/doi/10.1145/3368089.3417050)\n",
    "2. [A comprehensive guide to memory usage in PyTorch](https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor variable\n",
    "float32型（4byte）で0.25GiB次元の変数を考える。この変数は理論値で1GiBのサイズである。\n",
    "\n",
    "変数を定義または削除したときのメモリの使用量を確認して、実測値を求める。\n",
    "まず、Pytorchでの実測値を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: torch.float32, dim: 256.0MiB\n",
      "Initial: allocated = 0.0 MiB, reserved = 0.0MiB, max allocated = 0.0 MiB, used = 688.8 MiB\n",
      "Define: allocated = 1024.0 MiB, reserved = 1024.0MiB, max allocated = 1024.0 MiB, used = 2512.8 MiB\n",
      "Delete: allocated = 0.0 MiB, reserved = 1024.0MiB, max allocated = 1024.0 MiB, used = 2512.8 MiB\n",
      "Release: allocated = 0.0 MiB, reserved = 0.0MiB, max allocated = 1024.0 MiB, used = 1488.8 MiB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "import torch\n",
    "\n",
    "\n",
    "def print_memory_torch(prefix: str):\n",
    "    \"\"\"Print memory usage.\n",
    "    \"\"\"    \n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)    \n",
    "    memory_al = torch.cuda.memory_allocated()\n",
    "    memory_res = torch.cuda.memory_reserved()\n",
    "    memory_maxal = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    print(f\"{prefix}: allocated = {memory_al/1024**2:.1f} MiB, \"\n",
    "        f\"reserved = {memory_res/1024**2:.1f}MiB, \"\n",
    "        f\"max allocated = {memory_maxal/1024**2:.1f} MiB, \"\n",
    "        f\"used = {int(info.used)/1024**2:.1f} MiB\")\n",
    "\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = torch.zeros(dim, dtype=torch.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_memory_torch(\"Initial\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "var_gpu = var_cpu.to(\"cuda\")\n",
    "print_memory_torch(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_torch(\"Delete\")\n",
    "\n",
    "# Release cached memory.\n",
    "torch.cuda.empty_cache()\n",
    "print_memory_torch(\"Release\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1024MiB=1GiBのVRAMが確保されたので、実測値は理論値と一致した。\n",
    "delで変数を削除したあともメモリはreservedとして確保されており、torch.cuda.empty_cache()によって完全にメモリが解放された。\n",
    "\n",
    "次にTensorFlowでの実測値を求める。\n",
    "Tensorflowはデフォルトでプログラム開始時にメモリを最大限確保する。必要なメモリだけを確保するために、memory growthを有効にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:20:53.578757: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 23:20:53.599916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 23:20:54.028835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 23:20:54.494280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.494849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.494906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: float32, dim: 256.0MiB\n",
      "Initail: current = 0.0 MiB, peak = 0.0MiB, used = 1690.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:20:54.524291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.524393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.524438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.792496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.792581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.792631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 23:20:54.792682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21337 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define: current = 1024.0 MiB, peak = 1024.0MiB, used = 3737.2 MiB\n",
      "Delete: current = 0.0 MiB, peak = 1024.0MiB, used = 3737.2 MiB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_memory_tf(prefix: str):\n",
    "    \"\"\"Print memory usage.\n",
    "    \"\"\"\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)  \n",
    "    memory_info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "\n",
    "    print(f\"{prefix}: current = {memory_info['current']/1024**2:.1f} MiB, \"\n",
    "        f\"peak = {memory_info['peak']/1024**2:.1f}MiB, \"\n",
    "        f\"used = {int(info.used)/1024**2:.1f} MiB\")\n",
    "\n",
    "\n",
    "# Enabled memory growth.\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = np.zeros(dim, dtype=np.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "print_memory_tf(\"Initail\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "with tf.device(\"GPU\"): # type: ignore\n",
    "    var_gpu = tf.constant(var_cpu)\n",
    "print_memory_tf(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_tf(\"Delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1GiBのVRAMが確保されたので、実測値は理論値と一致した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "画像分類をする単純なCNNについて、メモリ使用量の理論値と実測値を比較する。\n",
    "\n",
    "データセットは疑似乱数で作成する。\n",
    "入力画像は3x224x224次元として、1280枚を用意する。\n",
    "分類クラスは10個として、one-hot encodingで表現する。\n",
    "バッチサイズは128とする。\n",
    "\n",
    "CNNは、畳み込み層、平均プーリング層、全結合層を1つずつ持つ構造とする。\n",
    "畳み込み層は、カーネル3x3、チャンネル数8、ストライド1、パディングなし、バイアスありとする。\n",
    "平均プーリング層は、カーネル2x2、ストライド2、パディングなしとする。\n",
    "全結合層は10次元の出力である。\n",
    "学習の最適化にはSGDを用いる。\n",
    "数値の精度はPytorchのデフォルトであるfloat32とする。\n",
    "\n",
    "このCNNに必要なメモリの理論値を求める。\n",
    "参考文献1,2によれば、学習と推論に必要なメモリは各々以下のようになる。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{usage for training}   &= \\text{data} + \\text{weight} + \\text{forward output} + \\text{weight gradient} + \\text{output gradient} \\\\\n",
    "                            &= D + W + O * b + W * (d + m) + (O + D) \\\\\n",
    "\\text{usage for inference}  &= \\text{data} + \\text{weight} + \\text{forward output} \\\\\n",
    "                            &= D + W + O \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "ここで、  \n",
    "$D$: ミニバッチのデータセットのメモリ使用量  \n",
    "$W$: 学習パラメーターのメモリ使用量  \n",
    "$O$: 中間層の出力のメモリ使用量  \n",
    "$b$: 推論時に中間層の出力のみ半精度にするMixed Precisionを利用する場合は0.5、そうでない場合は1  \n",
    "$d$: 複数のGPUで実行するDistributed Data Parallelを利用する場合は2、そうでない場合は1  \n",
    "$m$: 最適化で使うモーメントの数（SGD: 0, Adagrad, RMSprop: 1, Adam: 2）\n",
    "\n",
    "実際にはこれに加えて、GPUでの計算に必要なメモリ（CUDA context、cuDNN Workspace）とメモリ管理の最適化のための余剰メモリ（Internal Tensor Fragmentation）が使用される。文献1によれば、この追加分はライブラリーのバージョンやモデルによって変わるが、0.5GB程度である。\n",
    "\n",
    "今回のCNNの場合は、メモリ使用量の理論値は以下のように求まる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dim_input = [3,224,224]\n",
    "        self.dim_output = [10]\n",
    "        self.datasize = 512\n",
    "        self.batchsize = 128\n",
    "        self.num_epochs = 3\n",
    "        self.lr = 1e-2\n",
    "        self.device = 'cuda'\n",
    "        self.criterion = F.cross_entropy\n",
    "        self.optim = optim.SGD\n",
    "        self.moment = 0         # SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "        self.ddp = 1            # Distributed data parallel: 2, Not: 1\n",
    "        self.mixed_pre = 1      # Mixed precision: 0.5, Not: 1\n",
    "\n",
    "\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(MiB): 73.5\n",
      "Weight(MiB): 3.8\n",
      "Forward output(MiB): 240.6\n",
      "Weight gradient(MiB): 3.8\n",
      "Output gradient(MiB): 314.1\n",
      "Total for training(MiB): 635.8\n",
      "Total for inference(MiB): 317.9\n"
     ]
    }
   ],
   "source": [
    "def print_memory_estimate1(\n",
    "    dim_input: list[int], \n",
    "    dim_output: list[int], \n",
    "    num_param: int, \n",
    "    num_hidden_output: int, \n",
    "    moment: int, \n",
    "    ddp: int = 1, \n",
    "    mixed_pre: float = 1):\n",
    "    '''Print theoretical memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_input: Shape of input data including batch size. e.g. [batch size, channel, width, height]\n",
    "    dim_output: Shape of output data.\n",
    "    num_param: Number of trainable parameters.\n",
    "    num_hidden_output: Total number of hidden layer's output. Don't include in-place hidden layer that require no additional memory (Activations, Normalizations).\n",
    "    moment: Moment use for optimization. SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "    ddp: Multiple GPU use. Distributed data parallel: 2, Not: 1\n",
    "    mixed_pre: Forward outputs memory saving by Mixed precision: 0.5, Not: 1\n",
    "    '''\n",
    "    mem_data = (np.prod(dim_input) + np.prod(dim_output)) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_hidden_output * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "num_param = 3*3*3*8+8 + 8*((conf.dim_input[1]-2)//2)*((conf.dim_input[2]-2)//2)*10 + 10\n",
    "num_output_shape = np.prod([conf.batchsize, 8, (conf.dim_input[1]-2), (conf.dim_input[2]-2)]) \\\n",
    "                 + np.prod([conf.batchsize, 8, ((conf.dim_input[1]-2)//2), ((conf.dim_input[2]-2)//2)])\n",
    "\n",
    "print_memory_estimate1([conf.batchsize] + conf.dim_input, conf.dim_output,\n",
    "                        num_param, int(num_output_shape), conf.moment, conf.ddp, conf.mixed_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よって、理論値は636MiBである。\n",
    "\n",
    "次にpytorchでCNNを実行したときのメモリ使用量の実測値を調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: allocated = 0.0 MiB, reserved = 0.0MiB, max allocated = 0.0 MiB, used = 3737.2 MiB\n",
      "Model: allocated = 3.8 MiB, reserved = 22.0MiB, max allocated = 3.8 MiB, used = 3759.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 4/4 [00:00<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 97.8 MiB, reserved = 604.0MiB, max allocated = 530.9 MiB, used = 5019.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 4/4 [00:00<00:00, 108.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 97.8 MiB, reserved = 604.0MiB, max allocated = 530.9 MiB, used = 5019.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 4/4 [00:00<00:00, 109.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 97.8 MiB, reserved = 604.0MiB, max allocated = 530.9 MiB, used = 5019.2 MiB\n",
      "Final: allocated = 97.8 MiB, reserved = 604.0MiB, max allocated = 530.9 MiB, used = 5019.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dim_c: int, dim_h: int, dim_w: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=dim_c, out_channels=8, kernel_size=3)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(8*((dim_h-2)//2)*((dim_w-2)//2), 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(\n",
    "    model:nn.Module, \n",
    "    dim_input: list[int], \n",
    "    dim_output: list[int],\n",
    "    batchsize: int, \n",
    "    epoch: int, \n",
    "    criterion: Callable[..., Tensor],\n",
    "    optimizer = optim.SGD, \n",
    "    device: str = \"cuda\"):\n",
    "    \"\"\"Train model using random dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including data size. e.g. [data size, channel, width, height]\n",
    "    dim_output: \n",
    "    batchsize:\n",
    "    epoch:\n",
    "    optimizer:\n",
    "    device: \n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print_memory_torch(\"Initial\")\n",
    "\n",
    "    model.to(device)\n",
    "    print_memory_torch(\"Model\")\n",
    "    \n",
    "    data = [[torch.randn([batchsize] + dim_input[1:]), \n",
    "             torch.randn([batchsize] + dim_output)] \n",
    "             for _ in range(dim_input[0]//batchsize)]\n",
    "\n",
    "    criterion = F.cross_entropy\n",
    "    opt = optimizer(model.parameters(), lr=0.01)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        with tqdm(data) as pbar:\n",
    "            pbar.set_description(f'[Epoch {ep + 1}]')\n",
    "            for x, y in pbar:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "        print_memory_torch(\"Train\")\n",
    "    print_memory_torch(\"Final\")\n",
    "\n",
    "\n",
    "model_cnn = CNN(conf.dim_input[0], conf.dim_input[1], conf.dim_input[2])\n",
    "train(model_cnn, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "      conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを用意したときの実測値は3.8MiBであり、理論値と一致した。\n",
    "学習終了時の実測ピーク値は531MiBであり、理論値と19.8%の誤差があった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "pytorchでは、torchinfoを使うとネットワーク構造やパラメータ数、メモリ使用量の情報を取得・表示できる。この機能は理論値を計算する際に便利であるが、メモリ使用量の計算は上記理論式と異なる。\n",
    "\n",
    "具体的には、メモリ使用量＝入力データ＋順伝搬/逆伝搬サイズ＋Weightとしている。順伝搬/逆伝搬サイズForward/backward pass sizeは、リストに表示している中間層の出力サイズの合計を求め、gradientを考慮してその２倍をメモリ使用量としている。よって、メモリを必要としない中間層（ReLU, Dropoutなど）が多いモデルでは特に誤差が大きくなる。例えば、VGGでは特に顕著である。\n",
    "また、メモリ使用量の表示がMiBではなくMBである。（1MB = 1000^2byte, 1MiB = 1,024^2byte）\n",
    "\n",
    "今回のCNNでのtorchinfoの表示は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=======================================================================================================================================================================================================================\n",
      "CNN                                      [128, 3, 224, 224]        [128, 10]                 --                             --                   --                        --                        True\n",
      "├─Conv2d: 1-1                            [128, 3, 224, 224]        [128, 8, 222, 222]        224                         0.02%                   [3, 3]                    1,413,070,848             True\n",
      "├─AvgPool2d: 1-2                         [128, 8, 222, 222]        [128, 8, 111, 111]        --                             --                   2                         --                        --\n",
      "├─Linear: 1-3                            [128, 98568]              [128, 10]                 985,690                    99.98%                   --                        126,168,320               True\n",
      "=======================================================================================================================================================================================================================\n",
      "Total params: 985,914\n",
      "Trainable params: 985,914\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.54\n",
      "=======================================================================================================================================================================================================================\n",
      "Input size (MB): 77.07\n",
      "Forward/backward pass size (MB): 403.74\n",
      "Params size (MB): 3.94\n",
      "Estimated Total Size (MB): 484.76\n",
      "=======================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "result = summary(model_cnn, [conf.batchsize] + conf.dim_input,\n",
    "                depth=6,\n",
    "                col_names=[\"input_size\",\n",
    "                            \"output_size\",\n",
    "                            \"num_params\",\n",
    "                            \"params_percent\",\n",
    "                            \"kernel_size\",\n",
    "                            \"mult_adds\",\n",
    "                            \"trainable\"])\n",
    "print(result)\n",
    "\n",
    "# Confirm result\n",
    "# num_input = conf.batchsize * conf.dim_input[0] * conf.dim_input[1] * conf.dim_input[2]\n",
    "# num_param = conf.dim_input[0]*3*3*8 + 8 + 8*((conf.dim_input[1]-2)//2)*((conf.dim_input[2]-2)//2)*conf.dim_output[0] + conf.dim_output[0]\n",
    "# num_hidden_output = np.prod([conf.batchsize, 8, conf.dim_input[1]-2, conf.dim_input[2]-2]) + np.prod([conf.batchsize, conf.dim_output[0]])\n",
    "# \n",
    "# print(\"Total prams: \", num_param)\n",
    "# print(\"Input size(byte): \", num_input * 4)\n",
    "# print(\"Forward/backward(byte): \", 2 * num_hidden_output * 4) # https://github.com/sksq96/pytorch-summary/issues/51\n",
    "# print(\"Params size(byte): \", num_param * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchinfoを利用すれば、modelのパラメータ数や中間層出力の数を取得して、理論値を計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(MiB): 73.5\n",
      "Weight(MiB): 3.8\n",
      "Forward output(MiB): 240.6\n",
      "Weight gradient(MiB): 3.8\n",
      "Output gradient(MiB): 314.1\n",
      "Total for training(MiB): 635.8\n",
      "Total for inference(MiB): 317.9\n"
     ]
    }
   ],
   "source": [
    "def is_memoryless(class_name: str) -> bool:\n",
    "    ''' Return True if the class is memoryless type.\n",
    "    Activations, normalizations and dropouts are supposed to be performed in-place updates by default\n",
    "    and does not require additional memory.\n",
    "    '''\n",
    "    return any((class_name == \"ReLU\",\n",
    "                class_name == \"LeakyReLU\",\n",
    "                class_name == \"Sigmoid\",\n",
    "                class_name == \"Tanh\",\n",
    "                class_name == \"ELU\",\n",
    "                class_name == \"GLU\",\n",
    "                class_name == \"PReLU\",\n",
    "                class_name == \"GELU\",\n",
    "                class_name == \"Mish\",\n",
    "                class_name == \"Softmin\",\n",
    "                class_name == \"Softmax\",\n",
    "                class_name == \"Softmax2d\",\n",
    "                class_name == \"Dropout\",\n",
    "                class_name == \"Dropout1d\",\n",
    "                class_name == \"Dropout2d\",\n",
    "                class_name == \"Dropout3d\",\n",
    "                class_name == \"AlphaDropout\",\n",
    "                class_name == \"BatchNorm1d\",\n",
    "                class_name == \"BatchNorm2d\",\n",
    "                class_name == \"BatchNorm3d\",\n",
    "                class_name == \"LayerNorm\"))\n",
    "\n",
    "def print_memory_estimate2(\n",
    "    model: nn.Module, \n",
    "    dim_input: list[int], \n",
    "    moment: int, \n",
    "    ddp: int=1, \n",
    "    mixed_pre: float = 1):\n",
    "    '''Print theoretical memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including batch size. e.g. [batch size, channel, width, height]\n",
    "    moment: Moment use for optimization. SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "    ddp: Multiple GPU use. Distributed data parallel: 2, Not: 1\n",
    "    mixed_pre: Forward outputs memory saving by Mixed precision: 0.5, Not: 1\n",
    "    '''\n",
    "    info = summary(model, dim_input, verbose=0)\n",
    "    dim_output = info.summary_list[-1].output_size[1:]\n",
    "\n",
    "    num_param = 0\n",
    "    num_output_shape = 0\n",
    "    last_layer = len(info.summary_list) -1\n",
    "    # print(\"#, Class, Leaf, Memoryless, Output\")\n",
    "    for i, layer in enumerate(info.summary_list):\n",
    "        # print(f\"{i}, {layer.class_name}, {layer.is_leaf_layer}, {is_memoryless(layer.class_name)}, {layer.output_size}\")\n",
    "        if layer.is_leaf_layer:\n",
    "            num_param += layer.trainable_params\n",
    "            if i != last_layer and not is_memoryless(layer.class_name):\n",
    "                num_output_shape += np.prod(layer.output_size)\n",
    "        elif layer.class_name == \"MultiheadAttention\": # pytorch's multihead attention is not leaf layer but shuold be counted.\n",
    "            num_output_shape += np.prod(layer.output_size) * 5\n",
    "            num_param += layer.trainable_params\n",
    "\n",
    "    \n",
    "    mem_data = (np.prod(dim_input) + np.prod(dim_output)) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_output_shape * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "print_memory_estimate2(model_cnn, [conf.batchsize] + conf.dim_input, \n",
    "                       conf.moment, conf.ddp, conf.mixed_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Estimated ===\n",
      "Data(MiB): 73.5\n",
      "Weight(MiB): 512.4\n",
      "Forward output(MiB): 7378.5\n",
      "Weight gradient(MiB): 512.4\n",
      "Output gradient(MiB): 7452.0\n",
      "Total for training(MiB): 15928.7\n",
      "Total for inference(MiB): 7964.4\n"
     ]
    }
   ],
   "source": [
    "model_vgg16 = models.vgg16_bn(weights=None)\n",
    "model_vgg16.classifier[6] = nn.Linear(model_vgg16.classifier[6].in_features, 10) # Change the num of final output features to 10  # type: ignore \n",
    "# print(model_vgg16)\n",
    "\n",
    "# result = summary(model_vgg16, [conf.batchsize] + conf.dim_input,\n",
    "#                 depth=6,\n",
    "#                 col_names=[\"input_size\",\n",
    "#                             \"output_size\",\n",
    "#                             \"num_params\",\n",
    "#                             \"params_percent\",\n",
    "#                             \"kernel_size\",\n",
    "#                             \"mult_adds\",\n",
    "#                             \"trainable\"])\n",
    "# print(result)\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_vgg16, [conf.batchsize] + conf.dim_input, \n",
    "                       conf.moment, conf.ddp, conf.mixed_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Real ===\n",
      "Initial: allocated = 537.1 MiB, reserved = 678.0MiB, max allocated = 537.1 MiB, used = 5096.2 MiB\n",
      "Model: allocated = 537.1 MiB, reserved = 678.0MiB, max allocated = 537.1 MiB, used = 5096.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1125.1 MiB, reserved = 18752.0MiB, max allocated = 17231.9 MiB, used = 23200.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 4/4 [00:01<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1125.1 MiB, reserved = 18752.0MiB, max allocated = 17231.9 MiB, used = 23229.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 4/4 [00:01<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1125.1 MiB, reserved = 18752.0MiB, max allocated = 17231.9 MiB, used = 23269.1 MiB\n",
      "Final: allocated = 1125.1 MiB, reserved = 18752.0MiB, max allocated = 17231.9 MiB, used = 23269.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_vgg16, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "      conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16では、理論値が15929MiB、実測値が17232MiB、誤差は7.6%であった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Estimated ===\n",
      "Data(MiB): 73.5\n",
      "Weight(MiB): 89.8\n",
      "Forward output(MiB): 5525.8\n",
      "Weight gradient(MiB): 89.8\n",
      "Output gradient(MiB): 5599.3\n",
      "Total for training(MiB): 11378.0\n",
      "Total for inference(MiB): 5689.0\n"
     ]
    }
   ],
   "source": [
    "model_rn50 = models.resnet50(weights=None)\n",
    "model_rn50.fc = nn.Linear(model_rn50.fc.in_features, 10) # Change the num of final output features to 10\n",
    "# print(model_rn50)\n",
    "\n",
    "# result = summary(model_rn50, [conf.batchsize] + conf.dim_input,\n",
    "#                 depth=6,\n",
    "#                 col_names=[\"input_size\",\n",
    "#                             \"output_size\",\n",
    "#                             \"num_params\",\n",
    "#                             \"params_percent\",\n",
    "#                             \"kernel_size\",\n",
    "#                             \"mult_adds\",\n",
    "#                             \"trainable\"])\n",
    "# print(result)\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_rn50, [conf.batchsize] + conf.dim_input, \n",
    "                       conf.moment, conf.ddp, conf.mixed_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Real ===\n",
      "Initial: allocated = 1141.6 MiB, reserved = 7160.0MiB, max allocated = 1141.6 MiB, used = 11671.6 MiB\n",
      "Model: allocated = 1141.6 MiB, reserved = 7160.0MiB, max allocated = 1141.6 MiB, used = 11671.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 4/4 [00:00<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1307.0 MiB, reserved = 12982.0MiB, max allocated = 11738.2 MiB, used = 17493.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1307.0 MiB, reserved = 12982.0MiB, max allocated = 11738.2 MiB, used = 17493.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 4/4 [00:00<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1307.0 MiB, reserved = 12982.0MiB, max allocated = 11738.2 MiB, used = 17494.1 MiB\n",
      "Final: allocated = 1307.0 MiB, reserved = 12982.0MiB, max allocated = 11738.2 MiB, used = 17494.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_rn50, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "      conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50では、理論値が11378MiB、実測値が11738MiB、誤差は3.1%であった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visiont Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Estimated ===\n",
      "Data(MiB): 73.5\n",
      "Weight(MiB): 326.7\n",
      "Forward output(MiB): 8938.5\n",
      "Weight gradient(MiB): 326.7\n",
      "Output gradient(MiB): 9012.0\n",
      "Total for training(MiB): 18677.5\n",
      "Total for inference(MiB): 9338.7\n"
     ]
    }
   ],
   "source": [
    "model_vit = models.vit_b_16(weights=None)\n",
    "model_vit.heads[0] = nn.Linear(in_features=model_vit.heads[0].in_features, out_features=10)# Change the num of final output features to 10  # type: ignore \n",
    "# print(model_vit)\n",
    "\n",
    "# result = summary(model_vit, [conf.batchsize] + conf.dim_input,\n",
    "#                 depth=7,\n",
    "#                 col_names=[\"input_size\",\n",
    "#                             \"output_size\",\n",
    "#                             \"num_params\",\n",
    "#                             \"params_percent\",\n",
    "#                             \"kernel_size\",\n",
    "#                             \"mult_adds\",\n",
    "#                             \"trainable\"])\n",
    "# print(result)\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_vit, [conf.batchsize] + conf.dim_input, \n",
    "                       conf.moment, conf.ddp, conf.mixed_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Real ===\n",
      "Initial: allocated = 1563.3 MiB, reserved = 9086.0MiB, max allocated = 1563.3 MiB, used = 13580.6 MiB\n",
      "Model: allocated = 1563.3 MiB, reserved = 9086.0MiB, max allocated = 1563.3 MiB, used = 13580.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1967.2 MiB, reserved = 18142.0MiB, max allocated = 17218.1 MiB, used = 22608.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1967.2 MiB, reserved = 18142.0MiB, max allocated = 17218.1 MiB, used = 22571.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 4/4 [00:02<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1967.2 MiB, reserved = 18142.0MiB, max allocated = 17218.1 MiB, used = 22649.2 MiB\n",
      "Final: allocated = 1967.2 MiB, reserved = 18142.0MiB, max allocated = 17218.1 MiB, used = 22649.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_vit, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "      conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViTでは、理論値が18678MiB、実測値が17218MiB、誤差は8.5%であった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNETR\n",
    "UNETRについては２つの実装方法（pytorch, tensorflow）で実測値を測定する。\n",
    "まず、pytorchでUNETRのモデルを定義して、理論値と実測値を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.dim_input = [64, 64, 64]\n",
    "conf.dim_output = [2, 64, 64, 64]\n",
    "conf.batchsize = 8\n",
    "conf.datasize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Estimated ===\n",
      "Data(MiB): 10.0\n",
      "Weight(MiB): 353.3\n",
      "Forward output(MiB): 2425.0\n",
      "Weight gradient(MiB): 353.3\n",
      "Output gradient(MiB): 2435.0\n",
      "Total for training(MiB): 5576.6\n",
      "Total for inference(MiB): 2788.3\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets.unetr import UNETR\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_unetr = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=conf.dim_output[0],\n",
    "    img_size=conf.dim_input,\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(conf.device)\n",
    "\n",
    "\n",
    "# result = summary(model_unetr, [conf.batchsize, 1] + conf.dim_input,\n",
    "#                 depth=6,\n",
    "#                 col_names=[\"input_size\",\n",
    "#                             \"output_size\",\n",
    "#                             \"num_params\",\n",
    "#                             \"params_percent\",\n",
    "#                             \"kernel_size\",\n",
    "#                             \"mult_adds\",\n",
    "#                             \"trainable\"])\n",
    "# print(result)\n",
    "\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_unetr, [conf.batchsize, 1] + conf.dim_input, \n",
    "                       conf.moment, conf.ddp, conf.mixed_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Real ===\n",
      "Initial: allocated = 2247.1 MiB, reserved = 9262.0MiB, max allocated = 2247.1 MiB, used = 13765.3 MiB\n",
      "Model: allocated = 2247.1 MiB, reserved = 9262.0MiB, max allocated = 2247.1 MiB, used = 13765.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 4/4 [00:00<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 2645.8 MiB, reserved = 9268.0MiB, max allocated = 5177.9 MiB, used = 13771.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 4/4 [00:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 2647.4 MiB, reserved = 9268.0MiB, max allocated = 5177.9 MiB, used = 13771.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 4/4 [00:00<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 2647.4 MiB, reserved = 9268.0MiB, max allocated = 5177.9 MiB, used = 13771.3 MiB\n",
      "Final: allocated = 2647.4 MiB, reserved = 9268.0MiB, max allocated = 5177.9 MiB, used = 13771.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_unetr, [conf.datasize, 1] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "      conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNETRは理論値が8932MiB、pytorchによる実装で実測値が7260MiB、誤差は7.7%であった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
