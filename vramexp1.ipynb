{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU memory usage for deep learning image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的\n",
    "DLによる画像処理について、モデルの学習と実行に必要なGPUメモリー容量の理論値と実測値の差を検証する。\n",
    "\n",
    "### 方法\n",
    "以下の４つの項目について、メモリーの使用量の理論値と実測値を比較する。\n",
    "   \n",
    "1. 変数\n",
    "2. Forward Neural Network\n",
    "3. Convolution Neural Network\n",
    "4. Vision Transformer\n",
    "5. UNETR\n",
    "\n",
    "メモリーの使用量の実測値は、pytorchもしくはtensorflowの関数と、pynvml（nvidia  management library）の関数で測定する。\n",
    "pytorchやtensorflowの関数ではプログラム中で確保された容量が測定される。一方、nvmlではプログラム実行に必要なpythonやcudaなどのオーバーヘッドを含んだ容量が測定される。\n",
    "変数とUNETRについては２つの実装方法（pytorch, tensorflow）で実測値を測定する。どちらの実装方法でも実測値に大きな差はないことが期待される。\n",
    "\n",
    "### 結果\n",
    "|  | 理論値[MiB] | 実測値[MiB] | 実測値（pynvml）[MiB] |\n",
    "|---|---|---|---|\n",
    "| 変数 | 1024 | 1024 | 1832 (pytorch)<br> 2153 (tensorflow) |\n",
    "\n",
    "#### 検証環境\n",
    "- python 3.10\n",
    "- tensorflow 2.13.0\n",
    "- torch 2.0.1\n",
    "- NVIDIA driver 530.30.02\n",
    "- CUDA toolkit 11.8\n",
    "- cuDNN 8.9\n",
    "\n",
    "#### 参考文献\n",
    "1. [A comprehensive guide to memory usage in PyTorch](https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3)\n",
    "2. [Estimating GPU Memory Consumption of Deep Learning Models](https://2020.esec-fse.org/details/esecfse-2020-industry-papers/5/Estimating-GPU-Memory-Consumption-of-Deep-Learning-Models), [video](https://dl.acm.org/doi/10.1145/3368089.3417050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数\n",
    "float32型（4byte）で0.25GiB次元の変数は、理論値で1GiBのサイズである。\n",
    "この変数を定義または削除したときのメモリーの使用量を確認して、実測値を求める。\n",
    "\n",
    "まず、Pytorchでの実測値を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: torch.float32, dim: 256.0MiB\n",
      "Initial: allocated = 0.000 MiB, reserved = 0.000MiB, max allocated = 0.000 MiB, used = 3727.750 MiB\n",
      "Define: allocated = 1024.000 MiB, reserved = 1024.000MiB, max allocated = 1024.000 MiB, used = 4755.875 MiB\n",
      "Delete: allocated = 0.000 MiB, reserved = 1024.000MiB, max allocated = 1024.000 MiB, used = 4755.875 MiB\n",
      "Release: allocated = 0.000 MiB, reserved = 0.000MiB, max allocated = 1024.000 MiB, used = 3731.875 MiB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "import torch\n",
    "\n",
    "\n",
    "def print_memory_torch(prefix: str) -> None:\n",
    "# Print memory usage\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)    \n",
    "    memory_al = torch.cuda.memory_allocated()\n",
    "    memory_res = torch.cuda.memory_reserved()\n",
    "    memory_maxal = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    print(f\"{prefix}: allocated = {memory_al/1024**2:.3f} MiB, \"\n",
    "        f\"reserved = {memory_res/1024**2:.3f}MiB, \"\n",
    "        f\"max allocated = {memory_maxal/1024**2:.3f} MiB, \"\n",
    "        f\"used = {info.used/1024**2:.3f} MiB\")\n",
    "\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = torch.zeros(dim, dtype=torch.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_memory_torch(\"Initial\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "var_gpu = var_cpu.to(\"cuda\")\n",
    "print_memory_torch(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_torch(\"Delete\")\n",
    "\n",
    "# Release cached memory.\n",
    "torch.cuda.empty_cache()\n",
    "print_memory_torch(\"Release\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1024MiB=1GiBのVRAMが確保されたので、実測値は理論値と一致した。\n",
    "delで変数を削除したあともメモリーはreservedとして確保されており、torch.cuda.empty_cache()によって完全にメモリーが解放された。\n",
    "\n",
    "nvmlのusedの結果から、実際には変数のサイズ以上のメモリーが確保され、変数の削除後にもオーバーヘッド分が残っていた。\n",
    "\n",
    "次にTensorFlowでの実測値を求める。\n",
    "Tensorflowはデフォルトでプログラム開始時にメモリを最大限確保する。必要なメモリだけを確保するために、memory growthを有効にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 22:41:43.433478: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 22:41:43.455502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 22:41:43.901319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-03 22:41:44.373948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.379284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.379363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.400093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.400187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.400232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.655044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.655128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.655178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 22:41:44.655221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21351 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: float32, dim: 256.0MiB\n",
      "Initail: current = 0.0 MiB, peak = 0.0MiB, used = 1676.500 MiB\n",
      "Define: current = 1024.0 MiB, peak = 1024.0MiB, used = 3724.500 MiB\n",
      "Delete: current = 0.0 MiB, peak = 1024.0MiB, used = 3724.500 MiB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_memory_tf(prefix: str) -> None:\n",
    "# Print memory usage\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)  \n",
    "    memory_info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "\n",
    "    print(f\"{prefix}: current = {memory_info['current']/1024**2:.1f} MiB, \"\n",
    "        f\"peak = {memory_info['peak']/1024**2:.1f}MiB, \"\n",
    "        f\"used = {info.used/1024**2:.3f} MiB\")\n",
    "\n",
    "\n",
    "# Enabled memory growth.\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = np.zeros(dim, dtype=np.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "print_memory_tf(\"Initail\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "with tf.device(\"GPU\"):\n",
    "    var_gpu = tf.constant(var_cpu)\n",
    "print_memory_tf(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_tf(\"Delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1GiBのVRAMが確保されたので、実測値は理論値と一致した。\n",
    "オーバーヘッドも含めると、実際には変数のサイズ以上のメモリが確保され、変数を削除したあとも解放されなかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "画像分類をするFNNのメモリー使用量の理論値と実測値を比較する。\n",
    "\n",
    "データセットは、10クラス画像分類のためのCIFAR10を用いる。\n",
    "入力画像は32x32x3次元である。データセットの60000枚のうち、40000枚を学習に、残りを10000枚ずつ検証とテストに割り当てる。\n",
    "バッチサイズは32とし、学習の1エポックで40000/32=1250回パラメーターが更新される。\n",
    "\n",
    "FNNは２つの隠れ層を持つ構造とする。\n",
    "隠れ層は512次元とし、バイアスを持たせず、バッチ正規化を行う。\n",
    "出力層は分類するクラスと同じ数の10次元とし、バイアスを持たせる。\n",
    "活性化関数にはReLUを用いる。\n",
    "学習の最適化にはSGDを用いる。\n",
    "数値の精度はPytorchのデフォルトであるfloat32とする。\n",
    "\n",
    "このFNNに必要なメモリーの理論値を求める。\n",
    "参考文献1によれば、学習と推論に必要なメモリーは各々以下のようになる。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{usage for training}   &= \\text{model} + \\text{forward} + \\text{gradients} + \\text{gradient moments} \\\\\n",
    "                            &= m + f * \\text{batch size} * b + m * d + m * o \\\\\n",
    "\\text{usage for inference}  &= m \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、  \n",
    "$m$: モデルの学習のパラメーターのメモリー使用量  \n",
    "$f$: 入力層を含む各層の出力のメモリー使用量 \n",
    "$b$: 各層の出力のみ半精度にするmixed precisionを利用する場合は0.5、そうでない場合は1  \n",
    "$d$: 複数のGPUで実行する場合は2、そうでない場合は1  \n",
    "$o$: 最適化で使うモーメントの数（SGD: 0, Adagrad, RMSprop: 1, Adam: 2）\n",
    "\n",
    "FNNの場合は、  \n",
    "$m = 32*32*3*512 + 512*2 + 512*512 + 512*2 + 512*10+10$ （隠れ層1＋バッチ正規化1＋隠れ層2＋バッチ正規化2＋出力層）  \n",
    "$f = 512*32*2$ （次元数xバッチサイズx活性化関数の数）  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
