{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU memory usage for deep learning image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的\n",
    "DLによる画像処理について、モデルの学習と実行に必要なGPUメモリー容量の理論値と実測値の差を検証する。\n",
    "\n",
    "### 方法\n",
    "以下の４つの項目について、メモリの使用量の理論値と実測値を比較する。\n",
    "   \n",
    "1. 変数\n",
    "2. Convolutional Neural Network\n",
    "3. ResNet50\n",
    "4. Vision Transformer\n",
    "5. UNETR\n",
    "\n",
    "メモリの使用量の実測値は、pytorchもしくはtensorflowの関数と、nvml（nvidia  management library）の関数で測定する。\n",
    "pytorchやtensorflowの関数ではプログラム中で確保された容量が測定される。一方、nvmlではプログラム実行に必要なpythonやcudaなどのオーバーヘッドを含んだ容量が測定される。\n",
    "変数とUNETRについては２つの実装方法（pytorch, tensorflow）で実測値を測定する。どちらの実装方法でも実測値に大きな差はないことが期待される。\n",
    "\n",
    "### 結果\n",
    "|  | 理論値[MiB] | 実測値[MiB] | 誤差[%] |\n",
    "|---|---|---|---|\n",
    "| 変数 | 1024 | 1024 | 20 |\n",
    "| CNN | 636 | 531 |  |\n",
    "| ResNet50 |  |  |  |\n",
    "| ViT |  |  |  |\n",
    "| UNETR |  |  |  |\n",
    "\n",
    "#### 検証環境\n",
    "- python 3.10\n",
    "- tensorflow 2.13.0\n",
    "- torch 2.0.1\n",
    "- NVIDIA driver 530.30.02\n",
    "- CUDA toolkit 11.8\n",
    "- cuDNN 8.9\n",
    "\n",
    "#### 参考文献\n",
    "1. [Estimating GPU Memory Consumption of Deep Learning Models](https://2020.esec-fse.org/details/esecfse-2020-industry-papers/5/Estimating-GPU-Memory-Consumption-of-Deep-Learning-Models), [video](https://dl.acm.org/doi/10.1145/3368089.3417050)\n",
    "2. [A comprehensive guide to memory usage in PyTorch](https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数\n",
    "float32型（4byte）で0.25GiB次元の変数を考える。この変数は理論値で1GiBのサイズである。\n",
    "\n",
    "変数を定義または削除したときのメモリの使用量を確認して、実測値を求める。\n",
    "まず、Pytorchでの実測値を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: torch.float32, dim: 256.0MiB\n",
      "Initial: allocated = 0.0 MiB, reserved = 0.0MiB, max allocated = 0.0 MiB, used = 602.3 MiB\n",
      "Define: allocated = 1024.0 MiB, reserved = 1024.0MiB, max allocated = 1024.0 MiB, used = 2426.3 MiB\n",
      "Delete: allocated = 0.0 MiB, reserved = 1024.0MiB, max allocated = 1024.0 MiB, used = 2426.3 MiB\n",
      "Release: allocated = 0.0 MiB, reserved = 0.0MiB, max allocated = 1024.0 MiB, used = 1402.3 MiB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "import torch\n",
    "\n",
    "\n",
    "def print_memory_torch(prefix: str) -> None:\n",
    "    # Print memory usage\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)    \n",
    "    memory_al = torch.cuda.memory_allocated()\n",
    "    memory_res = torch.cuda.memory_reserved()\n",
    "    memory_maxal = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    print(f\"{prefix}: allocated = {memory_al/1024**2:.1f} MiB, \"\n",
    "        f\"reserved = {memory_res/1024**2:.1f}MiB, \"\n",
    "        f\"max allocated = {memory_maxal/1024**2:.1f} MiB, \"\n",
    "        f\"used = {info.used/1024**2:.1f} MiB\")\n",
    "\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = torch.zeros(dim, dtype=torch.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_memory_torch(\"Initial\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "var_gpu = var_cpu.to(\"cuda\")\n",
    "print_memory_torch(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_torch(\"Delete\")\n",
    "\n",
    "# Release cached memory.\n",
    "torch.cuda.empty_cache()\n",
    "print_memory_torch(\"Release\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1024MiB=1GiBのVRAMが確保されたので、実測値は理論値と一致した。\n",
    "delで変数を削除したあともメモリはreservedとして確保されており、torch.cuda.empty_cache()によって完全にメモリが解放された。\n",
    "\n",
    "nvmlのusedの結果から、実際には変数のサイズ以上のメモリが確保され、変数を削除した後にもオーバーヘッド分が残っていた。\n",
    "\n",
    "次にTensorFlowでの実測値を求める。\n",
    "Tensorflowはデフォルトでプログラム開始時にメモリを最大限確保する。必要なメモリだけを確保するために、memory growthを有効にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 21:50:55.782569: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-08 21:50:55.803908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 21:50:56.212819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-08 21:50:56.686291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.686847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.686904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.712043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.712134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.712174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.976121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.976203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.976243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 21:50:56.976285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21439 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cpu dtype: float32, dim: 256.0MiB\n",
      "Initail: current = 0.0 MiB, peak = 0.0MiB, used = 1588.3 MiB\n",
      "Define: current = 1024.0 MiB, peak = 1024.0MiB, used = 3634.8 MiB\n",
      "Delete: current = 0.0 MiB, peak = 1024.0MiB, used = 3634.8 MiB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_memory_tf(prefix: str) -> None:\n",
    "    # Print memory usage\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)  \n",
    "    memory_info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "\n",
    "    print(f\"{prefix}: current = {memory_info['current']/1024**2:.1f} MiB, \"\n",
    "        f\"peak = {memory_info['peak']/1024**2:.1f}MiB, \"\n",
    "        f\"used = {info.used/1024**2:.1f} MiB\")\n",
    "\n",
    "\n",
    "# Enabled memory growth.\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "# Define a variable with 1GiB.\n",
    "dim = 1024**3//4\n",
    "var_cpu = np.zeros(dim, dtype=np.float32)\n",
    "print(f\"var_cpu dtype: {var_cpu.dtype}, dim: {dim/1024**2}MiB\")\n",
    "\n",
    "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "print_memory_tf(\"Initail\")\n",
    "\n",
    "# Copy the variable to gpu.\n",
    "with tf.device(\"GPU\"):\n",
    "    var_gpu = tf.constant(var_cpu)\n",
    "print_memory_tf(\"Define\")\n",
    "\n",
    "# Delete the variable from gpu.\n",
    "del var_gpu\n",
    "print_memory_tf(\"Delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義した後に1GiBのVRAMが確保されたので、実測値は理論値と一致した。\n",
    "オーバーヘッドも含めると、実際には変数のサイズ以上のメモリが確保され、変数を削除したあとも解放されなかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "画像分類をする単純なCNNについて、メモリー使用量の理論値と実測値を比較する。\n",
    "\n",
    "データセットは疑似乱数で作成する。\n",
    "入力画像は3x224x224次元として、1280枚を用意する。\n",
    "分類クラスは10個として、one-hot encodingで表現する。\n",
    "バッチサイズは128とする。\n",
    "\n",
    "CNNは、畳み込み層、平均プーリング層、全結合層を1つずつ持つ構造とする。\n",
    "畳み込み層は、カーネル3x3、チャンネル数8、ストライド1、パディングなし、バイアスありとする。\n",
    "平均プーリング層は、カーネル2x2、ストライド2、パディングなしとする。\n",
    "全結合層は10次元の出力である。\n",
    "学習の最適化にはSGDを用いる。\n",
    "数値の精度はPytorchのデフォルトであるfloat32とする。\n",
    "\n",
    "このCNNに必要なメモリの理論値を求める。\n",
    "参考文献1,2によれば、学習と推論に必要なメモリは各々以下のようになる。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{usage for training}   &= \\text{data} + \\text{weight} + \\text{forward output} + \\text{weight gradient} + \\text{output gradient} \\\\\n",
    "                            &= D + W + O * b + W * (d + m) + (O + D) \\\\\n",
    "\\text{usage for inference}  &= \\text{data} + \\text{weight} + \\text{forward output} \\\\\n",
    "                            &= D + W + O \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "ここで、  \n",
    "$D$: ミニバッチのデータセットのメモリ使用量  \n",
    "$W$: 学習パラメーターのメモリ使用量  \n",
    "$O$: 中間層の出力のメモリ使用量  \n",
    "$b$: 推論時に中間層の出力のみ半精度にするMixed Precisionを利用する場合は0.5、そうでない場合は1  \n",
    "$d$: 複数のGPUで実行するDistributed Data Parallelを利用する場合は2、そうでない場合は1  \n",
    "$m$: 最適化で使うモーメントの数（SGD: 0, Adagrad, RMSprop: 1, Adam: 2）\n",
    "\n",
    "実際にはこれに加えて、GPUでの計算に必要なメモリー（CUDA context、cuDNN Workspace）とメモリ管理の最適化のための余剰メモリー（Internal Tensor Fragmentation）が使用される。文献1によれば、この追加分はライブラリーのバージョンやモデルによって変わるが、0.5GB程度である。\n",
    "\n",
    "今回のCNNの場合は、メモリー使用量の理論値は以下のように求まる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(MiB): 73.5\n",
      "Weight(MiB): 3.8\n",
      "Forward output(MiB): 240.6\n",
      "Weight gradient(MiB): 3.8\n",
      "Output gradient(MiB): 314.1\n",
      "Total for training(MiB): 635.8\n",
      "Total for inference(MiB): 317.9\n"
     ]
    }
   ],
   "source": [
    "def print_memory_theory1(\n",
    "    dim_input: list, \n",
    "    dim_output: list, \n",
    "    num_param: int, \n",
    "    num_output_shape: int, \n",
    "    moment: int, \n",
    "    ddp: int=1, \n",
    "    mixed_pre: float=1):\n",
    "    # Print theoretical memory usage\n",
    "    \n",
    "    batchsize\n",
    "    mem_data = (np.prod(dim_input) + np.prod(dim_output)) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_output_shape * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "batchsize = 128\n",
    "dim_input = [batchsize, 3, 224, 224]\n",
    "dim_output = [batchsize, 10]\n",
    "num_param = 3*3*3*8+8 + 8*((dim_input[2]-2)//2)*((dim_input[3]-2)//2)*10 + 10\n",
    "num_output_shape = np.prod([batchsize, 8, (dim_input[2]-2), (dim_input[3]-2)]) \\\n",
    "                 + np.prod([batchsize, 8, ((dim_input[2]-2)//2), ((dim_input[3]-2)//2)])\n",
    "moment = 0 # SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "d = 1 # Distributed data parallel: 2, Not: 1\n",
    "b = 1 # Mixed precision: 0.5, Not: 1\n",
    "\n",
    "print_memory_theory1(dim_input, dim_output, num_param, num_output_shape, moment, d, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よって、理論値は636MiBである。\n",
    "\n",
    "次にpytorchでCNNを実行したときのメモリ使用量の実測値を調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: allocated = 31.9 MiB, reserved = 42.0MiB, max allocated = 31.9 MiB, used = 4491.9 MiB\n",
      "Model: allocated = 35.7 MiB, reserved = 42.0MiB, max allocated = 35.7 MiB, used = 4493.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 10/10 [00:00<00:00, 105.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 114.0 MiB, reserved = 604.0MiB, max allocated = 547.2 MiB, used = 5051.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 10/10 [00:00<00:00, 103.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 114.0 MiB, reserved = 604.0MiB, max allocated = 547.2 MiB, used = 5049.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████| 10/10 [00:00<00:00, 107.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 114.0 MiB, reserved = 604.0MiB, max allocated = 547.2 MiB, used = 5049.1 MiB\n",
      "Final: allocated = 114.0 MiB, reserved = 604.0MiB, max allocated = 547.2 MiB, used = 5049.1 MiB\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dim_h: int, dim_w: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(8*((dim_h-2)//2)*((dim_w-2)//2), 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self.fc.weight.device\n",
    "\n",
    "def train(model:nn.Module, dim_input: list, batchsize: int, epoch: int=3, optimizer: optim.Optimizer=optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print_memory_torch(\"Initial\")\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "    print_memory_torch(\"Model\")\n",
    "\n",
    "    # dim_input = [1280, 3, 224, 224]\n",
    "    # model = Net(dim_input[2], dim_input[3])\n",
    "    # batchsize = dim_input[0]//10\n",
    "    data = [[torch.randn([batchsize, dim_input[1], dim_input[2], dim_input[3]]), \n",
    "             torch.randn(batchsize, 10)] \n",
    "             for _ in range(dim_input[0]//batchsize)]\n",
    "\n",
    "    criterion = F.cross_entropy\n",
    "    opt = optimizer(model.parameters(), lr=0.01)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        with tqdm(data) as pbar:\n",
    "            pbar.set_description(f'[Epoch {ep + 1}]')\n",
    "            for x, y in pbar:\n",
    "                x = x.to(model.get_device())\n",
    "                y = y.to(model.get_device())\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "        print_memory_torch(\"Train\")\n",
    "\n",
    "    print_memory_torch(\"Final\")\n",
    "\n",
    "model = CNN(dim_input[2], dim_input[3])\n",
    "train(model, dim_input=[1280, 3, 224, 224], batchsize=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを用意したときの実測値は3.8MiBであり、理論値と一致した。\n",
    "学習終了時の実測ピーク値は531MiBであり、理論値と20%の誤差があった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "pytorchでは、torchinfoを使うとネットワーク構造やパラメータ数、メモリー使用量の情報を取得・表示できる。この機能は理論値を計算する際に便利であるが、メモリー使用量の計算は上記理論式と異なる。\n",
    "\n",
    "具体的には、メモリー使用量＝入力データ＋順伝搬/逆伝搬サイズ＋Weightとしている。順伝搬/逆伝搬サイズForward/backward pass sizeは、学習する層の出力サイズの合計を求め、gradientを考慮してその２倍をメモリー使用量としている。よって、上記理論式とは全く異なる。\n",
    "また、メモリー使用量の表示がMiBではなくMBである。（1MB = 1000^2byte, 1MiB = 1,024^2byte）\n",
    "\n",
    "今回のCNNでのtorchinfoの表示は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #                   Kernel Shape\n",
      "===================================================================================================================\n",
      "CNN                                      [128, 10]                 --                        --\n",
      "├─Conv2d: 1-1                            [128, 8, 222, 222]        224                       [3, 3]\n",
      "├─AvgPool2d: 1-2                         [128, 8, 111, 111]        --                        2\n",
      "├─Linear: 1-3                            [128, 10]                 985,690                   --\n",
      "===================================================================================================================\n",
      "Total params: 985,914\n",
      "Trainable params: 985,914\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.54\n",
      "===================================================================================================================\n",
      "Input size (MB): 77.07\n",
      "Forward/backward pass size (MB): 403.74\n",
      "Params size (MB): 3.94\n",
      "Estimated Total Size (MB): 484.76\n",
      "===================================================================================================================\n",
      "Total prams:  985914\n",
      "Input size(byte):  77070336\n",
      "Forward/backward(byte):  403744768\n",
      "Params size(byte):  3943656\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "result = summary(model, (batchsize, dim_input[1], dim_input[2], dim_input[3]),\n",
    "        col_names=(\"output_size\", \"num_params\", \"kernel_size\"))\n",
    "print(result)\n",
    "\n",
    "# Confirm result\n",
    "num_input = batchsize * dim_input[1] * dim_input[2] * dim_input[3]\n",
    "num_param = 3*3*3*8+8 + 8*((dim_input[2]-2)//2)*((dim_input[3]-2)//2)*10 + 10\n",
    "num_output_shape = np.prod([128, 8, 222, 222]) + np.prod([128, 10])\n",
    "\n",
    "print(\"Total prams: \", num_param)\n",
    "print(\"Input size(byte): \", num_input * 4)\n",
    "print(\"Forward/backward(byte): \", 2 * num_output_shape * 4) # https://github.com/sksq96/pytorch-summary/issues/51\n",
    "print(\"Params size(byte): \", num_param * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchinfoを利用して、定義したmodelから理論値を計算できる。\n",
    "この方法では、パラメータ数や中間層出力の数を自動的に計算するので簡単である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(MiB): 73.5\n",
      "Weight(MiB): 3.8\n",
      "Forward output(MiB): 240.6\n",
      "Weight gradient(MiB): 3.8\n",
      "Output gradient(MiB): 314.1\n",
      "Total for training(MiB): 635.8\n",
      "Total for inference(MiB): 317.9\n"
     ]
    }
   ],
   "source": [
    "def print_memory_theory2(\n",
    "    model: nn.Module, \n",
    "    dim_input: list, \n",
    "    moment: int, \n",
    "    ddp: int=1, \n",
    "    mixed_pre: float=1):\n",
    "    # Print theoretical memory usage\n",
    "\n",
    "    info = summary(model, dim_input, verbose=0)\n",
    "    dim_output = info.summary_list[-1].output_size[1:]\n",
    "\n",
    "    num_param = 0\n",
    "    num_output_shape = 0\n",
    "    last_layer = len(info.summary_list) -1\n",
    "    for i, layer_info in enumerate(info.summary_list):\n",
    "        if layer_info.is_leaf_layer:\n",
    "            num_param += layer_info.trainable_params\n",
    "            if i != last_layer:\n",
    "                num_output_shape += np.prod(layer_info.output_size)\n",
    "    \n",
    "    mem_data = (np.prod(dim_input) + np.prod(dim_output)) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_output_shape * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "batchsize = 128\n",
    "dim_input = [batchsize, 3, 224, 224]\n",
    "moment = 0 # SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "d = 1 # Distributed data parallel: 2, Not: 1\n",
    "b = 1 # Mixed precision: 0.5, Not: 1\n",
    "\n",
    "print_memory_theory2(model, dim_input, moment, d, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
