{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリを使う層、使わない層を確かめる。\n",
    "畳み込み層にいろいろな層を付け足して、メモリ使用量を確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn, optim\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def print_memory_torch(prefix: str):\n",
    "    \"\"\"Print memory usage.\n",
    "    \"\"\"    \n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)    \n",
    "    memory_al = torch.cuda.memory_allocated()\n",
    "    memory_res = torch.cuda.memory_reserved()\n",
    "    memory_maxal = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    print(f\"{prefix}: allocated = {memory_al/1024**2:.1f} MiB, \"\n",
    "        f\"reserved = {memory_res/1024**2:.1f}MiB, \"\n",
    "        f\"max allocated = {memory_maxal/1024**2:.1f} MiB, \"\n",
    "        f\"used = {int(info.used)/1024**2:.1f} MiB\")\n",
    "    \n",
    "\n",
    "def is_memoryless(class_name: str) -> bool:\n",
    "    ''' Return True if the class is memoryless type.\n",
    "    Activations, normalizations and dropouts perform in-place updates by default\n",
    "    and does not require additional memory.\n",
    "    '''\n",
    "    return any((class_name == \"ReLU\",\n",
    "                class_name == \"Softmax2d\"))\n",
    "\n",
    "\n",
    "def print_memory_estimate2(\n",
    "    model: nn.Module, \n",
    "    dim_input: list[int], \n",
    "    moment: int, \n",
    "    ddp: int=1, \n",
    "    mixed_pre: float = 1):\n",
    "    '''Print theoretical memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including batch size. e.g. [batch size, channel, width, height]\n",
    "    moment: Moment use for optimization. SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "    ddp: Multiple GPU use. Distributed data parallel: 2, Not: 1\n",
    "    mixed_pre: Forward outputs memory saving by Mixed precision: 0.5, Not: 1\n",
    "    '''\n",
    "    info = summary(model, dim_input, verbose=0)\n",
    "    dim_output = info.summary_list[-1].output_size[1:]\n",
    "\n",
    "    num_param = 0\n",
    "    num_output_shape = 0\n",
    "    last_layer = len(info.summary_list) -1\n",
    "    # print(\"#, Class, Leaf, Memoryless, Output\")\n",
    "    for i, layer in enumerate(info.summary_list):\n",
    "        # print(f\"{i}, {layer.class_name}, {layer.is_leaf_layer}, {is_memoryless(layer.class_name)}, {layer.output_size}\")\n",
    "        if layer.is_leaf_layer:\n",
    "            num_param += layer.trainable_params\n",
    "            if not is_memoryless(layer.class_name):\n",
    "                num_output_shape += np.prod(layer.output_size)\n",
    "        elif layer.class_name == \"MultiheadAttention\": # pytorch's multihead attention is not leaf layer but shuold be counted.\n",
    "            num_output_shape += np.prod(layer.output_size) * 5\n",
    "            num_param += layer.trainable_params\n",
    "    \n",
    "    mem_data = np.prod(dim_input) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_output_shape * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    model:nn.Module, \n",
    "    dim_input: list[int], \n",
    "    dim_output: list[int],\n",
    "    batchsize: int, \n",
    "    epoch: int, \n",
    "    criterion: Callable[..., Tensor],\n",
    "    optimizer = optim.SGD, \n",
    "    device: str = \"cuda\"):\n",
    "    \"\"\"Train model using random dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including data size. e.g. [data size, channel, width, height]\n",
    "    dim_output: \n",
    "    batchsize:\n",
    "    epoch:\n",
    "    optimizer:\n",
    "    device: \n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print_memory_torch(\"Initial\")\n",
    "\n",
    "    model.to(device)\n",
    "    print_memory_torch(\"Model\")\n",
    "    \n",
    "    data = [[torch.randn([batchsize] + dim_input[1:]), \n",
    "             torch.randn([batchsize] + dim_output)] \n",
    "             for _ in range(dim_input[0]//batchsize)]\n",
    "\n",
    "    criterion = F.cross_entropy\n",
    "    opt = optimizer(model.parameters(), lr=0.01)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        with tqdm(data) as pbar:\n",
    "            pbar.set_description(f'[Epoch {ep + 1}]')\n",
    "            for x, y in pbar:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "        print_memory_torch(\"Train\")\n",
    "    print_memory_torch(\"Final\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # self.dim_input = [3,224,224]\n",
    "        # self.dim_output = [3,224,224]\n",
    "        # self.batchsize = 1000\n",
    "        self.datasize = 2000\n",
    "        self.num_epochs = 2\n",
    "        self.lr = 1e-2\n",
    "        self.device = 'cuda'\n",
    "        self.criterion = F.cross_entropy\n",
    "        self.optim = optim.SGD\n",
    "        self.moment = 0         # SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "        self.ddp = 1            # Distributed data parallel: 2, Not: 1\n",
    "        self.mixed_pre = 1      # Mixed precision: 0.5, Not: 1\n",
    "\n",
    "        # for MultiheadAttention\n",
    "        self.dim_input = [3,112,112]\n",
    "        self.dim_output = [3,112,112]\n",
    "        self.batchsize = 1\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_c: int, dim_h: int, dim_w: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=dim_c, out_channels=dim_c, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=dim_c, out_channels=dim_c, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn = nn.BatchNorm2d(num_features=dim_c)\n",
    "        self.dropout = nn.Dropout(inplace=False)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=dim_h * dim_w, num_heads=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.bn(x)\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.softmax(x)\n",
    "        x = x.view(x.size()[0], x.size()[1], x.size()[2] * x.size()[3])\n",
    "        x,_ = self.mha(x,x,x)\n",
    "        x = x.view(x.size()[0], x.size()[1], math.isqrt(x.size()[2]), math.isqrt(x.size()[2]))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=======================================================================================================================================================================================================================\n",
      "Net                                      [1, 3, 112, 112]          [1, 3, 112, 112]          90                          0.00%                   --                        --                        True\n",
      "├─Conv2d: 1-1                            [1, 3, 112, 112]          [1, 3, 112, 112]          84                          0.00%                   [3, 3]                    1,053,696                 True\n",
      "├─MultiheadAttention: 1-2                [1, 3, 12544]             [1, 3, 12544]             629,457,920               100.00%                   --                        --                        True\n",
      "=======================================================================================================================================================================================================================\n",
      "Total params: 629,458,094\n",
      "Trainable params: 629,458,094\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.05\n",
      "=======================================================================================================================================================================================================================\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 0.30\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.45\n",
      "=======================================================================================================================================================================================================================\n",
      "=== Estimated ===\n",
      "Data(MiB): 0.1\n",
      "Weight(MiB): 2401.2\n",
      "Forward output(MiB): 0.9\n",
      "Weight gradient(MiB): 2401.2\n",
      "Output gradient(MiB): 1.0\n",
      "Total for training(MiB): 4804.4\n",
      "Total for inference(MiB): 2402.2\n",
      "=== Real ===\n",
      "Initial: allocated = 2417.4 MiB, reserved = 2426.0MiB, max allocated = 2417.4 MiB, used = 4510.8 MiB\n",
      "Model: allocated = 2417.4 MiB, reserved = 2426.0MiB, max allocated = 2417.4 MiB, used = 4510.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 2000/2000 [00:36<00:00, 55.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 4819.1 MiB, reserved = 4832.0MiB, max allocated = 4819.8 MiB, used = 6957.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 2000/2000 [00:35<00:00, 55.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 4819.1 MiB, reserved = 4832.0MiB, max allocated = 4819.8 MiB, used = 6954.1 MiB\n",
      "Final: allocated = 4819.1 MiB, reserved = 4832.0MiB, max allocated = 4819.8 MiB, used = 6954.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf = Config()\n",
    "model_cnn = Net(conf.dim_input[0], conf.dim_input[1], conf.dim_input[2])\n",
    "\n",
    "result = summary(model_cnn, [conf.batchsize] + conf.dim_input,\n",
    "            depth=6,\n",
    "            col_names=[\"input_size\",\n",
    "                        \"output_size\",\n",
    "                        \"num_params\",\n",
    "                        \"params_percent\",\n",
    "                        \"kernel_size\",\n",
    "                        \"mult_adds\",\n",
    "                        \"trainable\"])\n",
    "print(result)\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_cnn, [conf.batchsize] + conf.dim_input, \n",
    "                    conf.moment, conf.ddp, conf.mixed_pre)\n",
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_cnn, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "    conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  | Estimated [MiB] | Real [MiB] |\n",
    "|---|---|---|\n",
    "| Conv | 2297 | 3445 |\n",
    "| Conv + Conv | 3445 | 4020 |\n",
    "| Conv + ReLU | 2297 | 3445 |\n",
    "| Conv + BachNorm | 3445 | 4020 |\n",
    "| Conv + Dropout | 3445 | 3589 |\n",
    "| Conv + Softmax | 2297 | 3445 |\n",
    "| MultiheadAttention | 4804 | 4820 |\n",
    "\n",
    "- ReLU、Softmaxはメモリを使っていない。\n",
    "- Dropoutはわずかな増加だった。出力サイズほどのメモリは使っていない。\n",
    "- ReLUのinplace=True, Falseの設定を変えても変化はなかった\n",
    "- BachNormはConvと同じだけ増加した\n",
    "- ConvやBatchNormを追加したときのメモリ使用量の増加は出力サイズの半分だった\n",
    "- MultiheadAttentionについてはGPUメモリ不足になるため、単独で実行し、別パラメータを使った。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
