{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリーを使う層、使わない層を確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn, optim\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def print_memory_torch(prefix: str):\n",
    "    \"\"\"Print memory usage.\n",
    "    \"\"\"    \n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)    \n",
    "    memory_al = torch.cuda.memory_allocated()\n",
    "    memory_res = torch.cuda.memory_reserved()\n",
    "    memory_maxal = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    print(f\"{prefix}: allocated = {memory_al/1024**2:.1f} MiB, \"\n",
    "        f\"reserved = {memory_res/1024**2:.1f}MiB, \"\n",
    "        f\"max allocated = {memory_maxal/1024**2:.1f} MiB, \"\n",
    "        f\"used = {int(info.used)/1024**2:.1f} MiB\")\n",
    "    \n",
    "\n",
    "def is_memoryless(class_name: str) -> bool:\n",
    "    ''' Return True if the class is memoryless type.\n",
    "    Activations, normalizations and dropouts perform in-place updates by default\n",
    "    and does not require additional memory.\n",
    "    '''\n",
    "    return any((class_name == \"ReLU\",\n",
    "                class_name == \"LeakyReLU\",\n",
    "                class_name == \"Sigmoid\",\n",
    "                class_name == \"Tanh\",\n",
    "                class_name == \"ELU\",\n",
    "                class_name == \"GLU\",\n",
    "                class_name == \"PReLU\",\n",
    "                class_name == \"GELU\",\n",
    "                class_name == \"Mish\",\n",
    "                class_name == \"Softmin\",\n",
    "                class_name == \"Softmax\",\n",
    "                class_name == \"Softmax2d\"))\n",
    "\n",
    "\n",
    "def print_memory_estimate2(\n",
    "    model: nn.Module, \n",
    "    dim_input: list[int], \n",
    "    moment: int, \n",
    "    ddp: int=1, \n",
    "    mixed_pre: float = 1):\n",
    "    '''Print theoretical memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including batch size. e.g. [batch size, channel, width, height]\n",
    "    moment: Moment use for optimization. SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "    ddp: Multiple GPU use. Distributed data parallel: 2, Not: 1\n",
    "    mixed_pre: Forward outputs memory saving by Mixed precision: 0.5, Not: 1\n",
    "    '''\n",
    "    info = summary(model, dim_input, verbose=0)\n",
    "    dim_output = info.summary_list[-1].output_size[1:]\n",
    "\n",
    "    num_param = 0\n",
    "    num_output_shape = 0\n",
    "    last_layer = len(info.summary_list) -1\n",
    "    # print(\"#, Class, Leaf, Memoryless, Output\")\n",
    "    for i, layer in enumerate(info.summary_list):\n",
    "        # print(f\"{i}, {layer.class_name}, {layer.is_leaf_layer}, {is_memoryless(layer.class_name)}, {layer.output_size}\")\n",
    "        if layer.is_leaf_layer:\n",
    "            num_param += layer.trainable_params\n",
    "            if i != last_layer and not is_memoryless(layer.class_name):\n",
    "                num_output_shape += np.prod(layer.output_size)\n",
    "    \n",
    "    mem_data = (np.prod(dim_input) + np.prod(dim_output)) * 4\n",
    "    mem_weight = num_param * 4\n",
    "    mem_weight_grad = mem_weight * (ddp + moment)\n",
    "    mem_forward_output = num_output_shape * 4 * mixed_pre\n",
    "    mem_output_gradient = mem_forward_output + mem_data\n",
    "    mem_training = mem_data + mem_weight + mem_forward_output + mem_weight_grad + mem_output_gradient\n",
    "    mem_inference = mem_data + mem_weight + mem_forward_output\n",
    "\n",
    "    print(f\"Data(MiB): {mem_data/1024**2:.1f}\")\n",
    "    print(f\"Weight(MiB): {mem_weight/1024**2:.1f}\")\n",
    "    print(f\"Forward output(MiB): {mem_forward_output/1024**2:.1f}\")\n",
    "    print(f\"Weight gradient(MiB): {mem_weight_grad/1024**2:.1f}\")\n",
    "    print(f\"Output gradient(MiB): {mem_output_gradient/1024**2:.1f}\")\n",
    "    print(f\"Total for training(MiB): {mem_training/1024**2:.1f}\")\n",
    "    print(f\"Total for inference(MiB): {mem_inference/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    model:nn.Module, \n",
    "    dim_input: list[int], \n",
    "    dim_output: list[int],\n",
    "    batchsize: int, \n",
    "    epoch: int, \n",
    "    criterion: Callable[..., Tensor],\n",
    "    optimizer = optim.SGD, \n",
    "    device: str = \"cuda\"):\n",
    "    \"\"\"Train model using random dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "    dim_input: Shape of input data including data size. e.g. [data size, channel, width, height]\n",
    "    dim_output: \n",
    "    batchsize:\n",
    "    epoch:\n",
    "    optimizer:\n",
    "    device: \n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print_memory_torch(\"Initial\")\n",
    "\n",
    "    model.to(device)\n",
    "    print_memory_torch(\"Model\")\n",
    "    \n",
    "    data = [[torch.randn([batchsize] + dim_input[1:]), \n",
    "             torch.randn([batchsize] + dim_output)] \n",
    "             for _ in range(dim_input[0]//batchsize)]\n",
    "\n",
    "    criterion = F.cross_entropy\n",
    "    opt = optimizer(model.parameters(), lr=0.01)\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        with tqdm(data) as pbar:\n",
    "            pbar.set_description(f'[Epoch {ep + 1}]')\n",
    "            for x, y in pbar:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "        print_memory_torch(\"Train\")\n",
    "    print_memory_torch(\"Final\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=======================================================================================================================================================================================================================\n",
      "Net                                      [1000, 3, 224, 224]       [1000, 3, 224, 224]       --                             --                   --                        --                        True\n",
      "├─Conv2d: 1-1                            [1000, 3, 224, 224]       [1000, 3, 224, 224]       84                         48.28%                   [3, 3]                    4,214,784,000             True\n",
      "├─BatchNorm2d: 1-2                       [1000, 3, 224, 224]       [1000, 3, 224, 224]       6                           3.45%                   --                        6,000                     True\n",
      "├─ReLU: 1-3                              [1000, 3, 224, 224]       [1000, 3, 224, 224]       --                             --                   --                        --                        --\n",
      "├─Dropout: 1-4                           [1000, 3, 224, 224]       [1000, 3, 224, 224]       --                             --                   --                        --                        --\n",
      "├─Softmax2d: 1-5                         [1000, 3, 224, 224]       [1000, 3, 224, 224]       --                             --                   --                        --                        --\n",
      "├─Conv2d: 1-6                            [1000, 3, 224, 224]       [1000, 3, 224, 224]       84                         48.28%                   [3, 3]                    4,214,784,000             True\n",
      "=======================================================================================================================================================================================================================\n",
      "Total params: 174\n",
      "Trainable params: 174\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 8.43\n",
      "=======================================================================================================================================================================================================================\n",
      "Input size (MB): 602.11\n",
      "Forward/backward pass size (MB): 3612.67\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 4214.78\n",
      "=======================================================================================================================================================================================================================\n",
      "=== Estimated ===\n",
      "Data(MiB): 574.8\n",
      "Weight(MiB): 0.0\n",
      "Forward output(MiB): 1722.7\n",
      "Weight gradient(MiB): 0.0\n",
      "Output gradient(MiB): 2297.4\n",
      "Total for training(MiB): 4594.9\n",
      "Total for inference(MiB): 2297.4\n",
      "=== Real ===\n",
      "Initial: allocated = 0.0 MiB, reserved = 2.0MiB, max allocated = 0.0 MiB, used = 15152.8 MiB\n",
      "Model: allocated = 0.0 MiB, reserved = 2.0MiB, max allocated = 0.0 MiB, used = 15152.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████| 2/2 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1722.7 MiB, reserved = 5330.0MiB, max allocated = 5312.0 MiB, used = 20550.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|██████████| 2/2 [00:00<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: allocated = 1722.7 MiB, reserved = 5330.0MiB, max allocated = 5312.0 MiB, used = 20549.8 MiB\n",
      "Final: allocated = 1722.7 MiB, reserved = 5330.0MiB, max allocated = 5312.0 MiB, used = 20549.8 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dim_input = [3,224,224]\n",
    "        self.dim_output = [3,224,224]\n",
    "        self.datasize = 2000\n",
    "        self.batchsize = 1000\n",
    "        self.num_epochs = 2\n",
    "        self.lr = 1e-2\n",
    "        self.device = 'cuda'\n",
    "        self.criterion = F.cross_entropy\n",
    "        self.optim = optim.SGD\n",
    "        self.moment = 0         # SGD: 0, Adagrad, RMSprop: 1, Adam: 2\n",
    "        self.ddp = 1            # Distributed data parallel: 2, Not: 1\n",
    "        self.mixed_pre = 1      # Mixed precision: 0.5, Not: 1\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_c: int, dim_h: int, dim_w: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=dim_c, out_channels=dim_c, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=dim_c)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(inplace=False)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "        self.conv2 = nn.Conv2d(in_channels=dim_c, out_channels=dim_c, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "conf = Config()\n",
    "model_cnn = Net(conf.dim_input[0], conf.dim_input[1], conf.dim_input[2])\n",
    "\n",
    "result = summary(model_cnn, [conf.batchsize] + conf.dim_input,\n",
    "            depth=6,\n",
    "            col_names=[\"input_size\",\n",
    "                        \"output_size\",\n",
    "                        \"num_params\",\n",
    "                        \"params_percent\",\n",
    "                        \"kernel_size\",\n",
    "                        \"mult_adds\",\n",
    "                        \"trainable\"])\n",
    "print(result)\n",
    "\n",
    "print(\"=== Estimated ===\")\n",
    "print_memory_estimate2(model_cnn, [conf.batchsize] + conf.dim_input, \n",
    "                    conf.moment, conf.ddp, conf.mixed_pre)\n",
    "\n",
    "print(\"=== Real ===\")\n",
    "train(model_cnn, [conf.datasize] + conf.dim_input, conf.dim_output, conf.batchsize, conf.num_epochs,\n",
    "    conf.criterion, conf.optim, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  | Estimated [MiB] | Real [MiB] |\n",
    "|---|---|---|\n",
    "| Conv | 1150 | 3445 |\n",
    "| +Conv | 2298 | 4020 |\n",
    "| +ReLU | 2298 | 4020 |\n",
    "| +BachNorm | 3447 | 4594 |\n",
    "| +Dropout | 4595 | 5312 |\n",
    "| +Softmax | 4595 | 5312 |\n",
    "\n",
    "- ReLU、Softmaxを追加しても変化はなかった\n",
    "- ReLUのinplace=True, Falseの設定を変えても変化はなかった\n",
    "- 要素を加えていったとき、理論的な増加分1GiBよりも、増加は少なかった\n",
    "  - そもそも、理論的な増加分はいくらかよくわかっていない？\n",
    "  - 計算するためにメモリを使っているだけで、Output gradientを生成しているわけではない？つまり理論的には加算不要？\n",
    "- Conv単独のときが最も誤差が大きく、要素が増えると誤差が減った\n",
    "  - 余分に確保されるメモリー（増加分）と再利用されるメモリー（減少分）のバランスが変わるからだと思われる。要素が多いほど再利用されるメモリーが増えて、減少方向に修正されていく？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
